= Troubleshooting

== Introduction

For this section of the lab, we will be covering common issues that occur when performing a Bare Metal Deployment and how to triage these issues.

[[boot_source_na]]
== Boot Source Not Available

We have run into issues in the past where the downloaded Boot Sources from Red Hat existed one day and then seemed to have gone missing the next day.  The confusing part happens when you look and see the PVC for the disk image, but it still says the Boot Source is missing.  To troubleshoot this, we need to walk through how the volume is associated with the templated.  Start by looking at the DataSource that is associated with one of the Templates that has gone missing.  By default, the Template definition has the following for the location of the DataSource:

```
    spec:
      dataVolumeTemplates:
        - apiVersion: cdi.kubevirt.io/v1beta1
          kind: DataVolume
          metadata:
            name: '${NAME}'
          spec:
            sourceRef:
              kind: DataSource
              name: '${DATA_SOURCE_NAME}'
              namespace: '${DATA_SOURCE_NAMESPACE}'
            storage:
              resources:
                requests:
                  storage: 30Gi
...
parameters:
  - name: NAME
    description: VM name
    generate: expression
    from: 'centos7-[a-z0-9]{16}'
  - name: DATA_SOURCE_NAME
    description: Name of the DataSource to clone
    value: centos7
  - name: DATA_SOURCE_NAMESPACE
    description: Namespace of the DataSource
    value: openshift-virtualization-os-images
...
```

We see here we need to look at the `centos7` DataSource name in the `opnshift-virtualization-os-images` namespace. Navigate to `Administration->CustomResourceDefinitions` and search for `DataSource` and select that Custom Resource Definition (CRD).  Click `Instances` and find the associated DataVolume for the Template and click on it to open the DataSource. Switch to the `YAML` view and look at the spec section:

```
spec:
  source:
    snapshot:
      name: centos7-02aa45fbcbad
      namespace: openshift-virtualization-os-images
```

Here we can see the DataSource is actually a `snapshot` and not a `pvc`.  Next, see if that `snapshot` exists by navigating to `Storage->VolumeSnapshots` and looking for the associated snapshot.  Ensure that you are looking in the `openshift-virtualization-os-images` namespace.  If you do not see this here, however, you do see a `pvc` with the same name, then the issue is that something changed in the environment (Operator update?) that caused a disassociation of the DataSource to the Template.

To fix this, note the StorageClass that the `pvc` exists on and navigated to `Administration->CustomResourceDefinitions` and search for `StorageProfile`.  You may see more than one, select the one in the `cdi.kubervirt.io` Group. Click `Instances` and then select the StorageProfile with the same name as the StorageClass for that `pvc.` In the spec section, change or add `dataImportCronSourceFormat` to match how the images are actually stored in your environment, either `pvc` or `snapshot`.

```
spec:
  dataImportCronSourceFormat: pvc
```

Once this is corrected, then all of the Boot Sources should now once again show as Available.

[[network_connectivity]]
== Network Connectivity

Troubleshooting network connectivity issues from your Virtual Machine could sometimes be difficult.  This module will cover some common troubleshooting techniques we have used, however, is far from a complete knowledgebase for debugging connectivity issues.  If you are having issues and suspect that they are network related, here's several things you can look at:

=== General Network Connection Issues Post-Deployment

In the lab, login to the bastion host:

`sudo ssh root@192.168.123.100`

In a real world deployment, you might try to open a debug container on one of the nodes:

```
[root@ocp4-bastion ~]# oc debug node/ocp4-worker1.aio.example.com
Temporary namespace openshift-debug-4cr4f is created for debugging node...
Starting pod/ocp4-worker1aioexamplecom-debug-8k9qr ...
To use host binaries, run `chroot /host`
Pod IP: 192.168.123.104
If you don't see a command prompt, try pressing enter.
sh-4.4# chroot /host
sh-5.1# 
```

However, sometimes you may have issues on your node preventing containers from being created. For these issues, you can try to SSH to the node using the public key you injected at deployment time.  This key gets added to the `core` user on each node.  In the real world, you would know the IP addresses of the deployed hosts, for this lab, your OCP Cluster is deployed with the following IP Addresses:

[%header,cols="1,1"]
|===
| NODE | IP
|ocp4-master1.aio.example.com | 192.168.123.101
|ocp4-master2.aio.example.com | 192.168.123.102
|ocp4-master3.aio.example.com | 192.168.123.103
|ocp4-worker1.aio.example.com | 192.168.123.104
|ocp4-worker2.aio.example.com | 192.168.123.105
|ocp4-worker3.aio.example.com | 192.168.123.106
|===

From the bastion host, go ahead and try to ssh to one of the nodes as the `core`` user.

```
[root@ocp4-bastion ~]# ssh core@192.168.123.104
Red Hat Enterprise Linux CoreOS 415.92.202407231021-0
  Part of OpenShift 4.15, RHCOS is a Kubernetes native operating system
  managed by the Machine Config Operator (`clusteroperator/machine-config`).

WARNING: Direct SSH access to machines is not recommended; instead,
make configuration changes via `machineconfig` objects:
  https://docs.openshift.com/container-platform/4.15/architecture/architecture-rhcos.html

---
Last login: Tue Aug 13 17:43:12 2024 from 192.168.123.100
[core@ocp4-worker1 ~]$ 
```
At this point, it's Linux troubleshooting skills you're going to rely on first.  Check the basics:

* NetworkManager is enabled, start there
* You should have a `br-ex` `ovs-bridge` interface that is used by Openshift Container Platform
* Confirm all of your MTU sizes are correct.  They should be the same for the interface, any bond/vlan and the bridges.
* Check the routes, ensure you have a default route and can ping it

RHCOS is a very minimal image and lacks troubleshooting tools such as  `tcpdump`.  You can gain access to this by executing the command `toolbox` at the command prompt on any node.  This will launch a `toolbox` container and connect you to it which has some useful troubleshooting tools, `tcpdump` among them.

=== Virtual Machine Network Connection Issues

Your Virtual Machines are attached to a Network Attachment Devices (NAD).  These Network Attachment Devices are then associated with a Node Network Configuration Policy (NNCP).  The Node Network Configuration Policy defines the connection from the Openshift Software Defined Network (OVN) to the physical interfaces on the hardware nodes. Network Attachment Definitions and Node Network Configuration Policies created in the `default` project are accessible by all other projects.
[NOTE]
====
Currently, only users with `Cluster Admin` rights may create Node Network Configuration Policies.
====
In the lab, a Node Network Configuration Policy named `ovs-br-flat` is already created in the `default` project. Examining this object shows the following spec:

```
spec:
  desiredState:
    interfaces:
      - bridge:
          options:
            stp: false
          port:
            - name: enp3s0
        description: An OVS with enp3s0 uplink
        name: ovs-br
        state: up
        type: ovs-bridge
    ovn:
      bridge-mappings:
        - bridge: ovs-br
          localnet: vm-network
          state: present
  nodeSelector:
    node-role.kubernetes.io/worker: ''
```

Here we see that a bridge named `ovs-br` is created off of physical interface `enp3s0`.  Within OVN, there is a mapping from that bridge to a localnet named `vm-network`.
The things to note here are:

* Interface names
* Bridge Name