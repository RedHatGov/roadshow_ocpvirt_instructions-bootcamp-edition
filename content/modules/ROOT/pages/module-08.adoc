= OpenShift API for Data Protection

The OpenShift API for Data Protection (OADP) product safeguards customer applications on OpenShift Container Platform. It offers comprehensive disaster recovery protection, covering OpenShift Container Platform applications, application-related cluster resources, persistent volumes, and internal images. OADP is also capable of backing up both containerized applications and virtual machines (VMs).

However, OADP does not serve as a disaster recovery solution for `etcd` or OpenShift Operators.

This workshop segment will explore the configuration of OADP to store the backups in an S3 bucket, the OADP Data Protection Application and completing a backup and restore of a virtual machine.

[[objectbucketclaim]]
== Object Bucket Claim

OADP stores the backups in an S3 bucket.  For this lab we are going to use internal storage but for production environments the recommendation is to use an external S3 destination.  This can be through the MultiCloud Gateway or a direct connection to a cloud provider or to an on-premise S3 instance. This section reviews the S3 object bucket claim that already exists and has been configured in the OADP Data Protection Application.

. To view the bucket, expand *Storage* in the left navigation pane.  Click *Object Storage*.  From the horizontal menu, click *Object Bucket Claim*.  Ensure that _Project_ is set to All Projects.
[NOTE]
If the left navigation pane does not have the _Data Foundation_ or _Object Storage_ options available under the Storage section, expand *Operator Hub*, select *Installed Operators*, and then select *OpenShift Data Foundation*.  In the _Details_ section of the operator, on the right, click the pencil under _Console plugin_ to enable it.  
+
image::module-08/01_obc.png[link=self, window=blank, width=100%]
+
. Select the *obc-backups* bucket claim.  Scroll to the bottom of the screen and click the _Reveal Values_ link. The AWS Access Key and AWS Secret Access Key are used to create an additional secret in the _openshift-adp_ namespace called _cloud-credentials_. 
+
image::module-08/02_revealobc.png[link=self, window=blank, width=100%]
+
. To view the cloud secret, in the left navigation pane, expand *Workloads*  and select *Secrets*.  Ensure you are in the *openshift-adp* project.  Click the cloud-credentials secret from the list.
+
image::module-08/03_cloudcreds.png[link=self, window=blank, width=100%]
+
. At the bottom, the cloud key data can be seen by clicking the _Reveal values_ link.  The cloud data contains the bucket's AWS Access Key and Secret Access Key as seen in the OBC.
+
image::module-08/04_cloudsecret.png[link=self, window=blank, width=100%]
+
[NOTE]
The cloud-credentials with the cloud key are required for the OADP Data Protection Application.

[[oadp-dpa]]
== OADP Data Protection Application

The Data Protection Application represents the configuration to safely backup and restore, perform disaster recovery and migrate Kubernetes cluster resources and persistent volumes.

. To view the data protection application, expand *Operators* in the left navigation pane, select *Installed Operators* and from the list operators presented, select *OADP Operator*.
+
image::module-08/05_oadpoperator.png[link=self, window=blank, width=100%]
+
. Scroll through the top horizontal menu bar to the far right to select *DataProtectionApplication*.  This will list the DPA that has already been defined.
+
image::module-08/06_oadp-dpa.png[link=self, window=blank, width=100%]
+
. Select the *oadp-dpa* DPA.  Click the *YAML* view from the top menu to view the configuration of the Data Protection Application.
+
Note the following configuration items:
+
* s3URL: 'http://s3.openshift-storage.svc';  This is the service for the MCG endpoint.  This can be updated as necessary when using an external S3 endpoint.  
* credentials key and name: cloud;  This is the cloud credentials that were viewed in the previous section of the lab.  
* bucket: backups-04511414-67b4-48bb-91f5-b02f2303ead2;   This is the .spec.bucketName of the OBC resource, again viewed in the previous section.  
* defaultPlugins: kubebvirt;   This plugin is needed to be able to complete the snapshots of the virtual machines.  
+
image::module-08/07_dpa-yaml.png[link=self, window=blank, width=100%]
+
[NOTE] 
The state of the DPA should be _Reconciled_.  If the DPA is not reconciled, backups cannot completed.  When the DPA is reconciled, a _BackupStorageLocation_ will have been created and should be in an available state.

. Scroll to the left on the horizontal menu bar to select *BackupStorageLocation*.  Note the name is the same as the DPA with _-1_ appended to the end and is in an available state.
+
image::module-08/08_bsl.png[link=self, window=blank, width=100%]


[[snapshot_class]]
== Modifying the Volume Snapshot Class

Setting a _DeletionPolicy_ of `Retain` on the _VolumeSnapShotClass_ will preserve the volume snapshot in the storage system for the lifetime of the backup and will prevent the deletion of the volume snapshot in the storage system  The CSI plugin for OADP, will choose the _VolumeSnapshotClass_ in the cluster that has the same driver name and also has the `velero.io/csi-volumesnapshot-class: true` label set.  This is needed, especially if the snapshot data is not moved to the S3 bucket using the Kopia Data Mover.

. To verify the _DeletionPolicy_, expand *Storage* in the left navigation pane, select *VolumeSnaphotClasses*.  From the list of classes, select the _ocs-storagecluster-rbdplugin-snapclass_ snapshot class.
+
image::module-08/11_volumesnapshotclass.png[link=self, window=blank, width=100%]
+
. Click the *YAML* option from the top horizontal menu. Update the YAML configuration to set *DeletionPolicy* to *Retain*.  Click *Save*.
+
image::module-08/12_rbdplugin-yaml.png[link=self, window=blank, width=100%]
+
[NOTE]
If the snapshots are moved to the S3 bucket using the Kopia Data Mover, this parameter becomes less important.  But to ensure snapshots that are taken by OADP are retained for the lifecycle of the backup, this parameter should be set to *Retain*.


[[backups]]
== Creating Backups  

After verifying the state of the OBC, the DPA, the BSL, and the deletion policy, we can create a backup.

. Expand *Operators* in the left navigation pane, select *Installed Operators*.  From the list of operators, select *OADP Operator*.
+
image::module-08/05_oadpoperator.png[link=self, window=blank, width=100%]
+
. Select *Backup* from the top horizontal menu.
+
image::module-08/09_backup.png[link=self, window=blank, width=100%]
+
. Click the *CreateBackup*.  Select the *YAML* view option.  
+
image::module-08/10_oadpBackup.png[link=self, window=blank, width=100%]
+
. You can copy and paste the yaml configuration below.  This example will backup the fedora-01 VM in _vmexamples_ namespace, including all metadata and PVCs/PVs defined, using snapshots for the PVCs and offload the snapshot to the S3 bucket using the built-in Kopia Data Mover. 

[source,sh,role=execute]
----
apiVersion: velero.io/v1
kind: Backup
metadata:
  generateName: fedora01-
  namespace: openshift-adp
spec:
  orLabelSelectors:
  - matchLabels:
      app: fedora-01
  - matchLabels:
      vm.kubevirt.io/name: fedora-01
  includedNamespaces:
    - vmexamples
  snapshotVolumes: true
  defaultVolumesToFsBackup: false
  includeClusterResources: false
  storageLocation: oadp-dpa-1
  snapshotMoveData: true
  ttl: 24h0m0s
----
[NOTE]
The *ttl* is the retention period of the backup.  If the snapshot data is not moved to the S3 bucket, the snapshots will be retained for the same retention period.
[TIP]
Use the *generateName:* feature to append random characters to the end of the given name to ensure the name is unique.

When the backup of all matching resources specified is complete the *Status* of the backup will be _Completed_.

image::module-08/13_backupComplete.png[link=self, window=blank, width=100%]

[[restore]]
== Restoring a Virtual Machine

Oh no! A disaster is about to strike.  

. Navigate to *Virtualization* in the left navigation pane, expand and select *Virutal Machines*.  Ensure you are in the *vmexamples* project.  Click on the three dots to the far right of the *fedora01* virtual machine, click *Delete* to stop the VM and delete it.
+
image::module-08/14_deleteVM.png[link=self, window=blank, width=100%]
+
. In the _Delete VirtualMachine_ pop-up window, confirm the correct VM name is displayed and the _Delete Disks_ checkbox is selected.  Click *Delete*.
+
image::module-08/15_deleteconfirm.png[link=self, window=blank, width=100%]
+
. You've realized you didn't mean to delete the VM, so let's restore it!  From the left navigation pane, expand *Operators*, select *Installed Operators*.  From the list of operators, select *OADP Operator*.  Ensure you are viewing _All Projects_ or the _openshift_adp_ project.
+
image::module-08/05_oadpoperator.png[link=self, window=blank, width=100%]
+
. Select *Backup* from the top horizontal menu bar.  Make note of the backup name you want to use for the restore.
+
image::module-08/16_backupname.png[link=self, window=blank, width=100%]
+
. Scroll to the right on the top horizontal menu bar, click *Restore*, *Create Restore*.
+
image::module-08/17_restore.png[link=self, window=blank, width=100%]
+
. Click on the YAML view option.  You can copy and paste the yaml configuration below. Be sure to update the .spec.backupName to the backupName noted in the previous step.  
+
image::module-08/18_restoreyaml.png[link=self, window=blank, width=100%]
+
This example will restore the _fedora-01_ VM in the _vmexamples_ namespace, including all metadata and PVCs/PVs that were included in the backup. New PVC/PVs will be created and the snapshot data will be downloaded from the S3 bucket and written to the new PVs.  

[IMPORTANT]
Any CR that already exists is not modified or restored from a backup.  OADP manages this by attempting to create the resource and if the create fails, it moves on to the next resource.  Therefore all VMs in a single namespace can be backed up collectively and one VM can be restored without impacting the others.

[source,sh,role=execute]
----
apiVersion: velero.io/v1
kind: Restore
metadata:
  generateName: restore-fedora01-
  namespace: openshift-adp
spec:
  includedNamespaces:
    - vmexamples
  excludedResources:
    - nodes
    - events
    - events.events.k8s.io
    - backups.velero.io
    - restores.velero.io
    - resticrepositories.velero.io
  backupName: 'fedora01-n2489'
  restorePVs: true
----

When the restore is complete the *Status* of the backup will be _Completed_.  

image::module-08/19_restorecomplete.png[link=self, window=blank, width=100%]


[[CLI_commands]]
== CLI Commands for Troubleshooting

You can use a shell command to access the Velero binary in the Velero deployment in the cluster.

[source,sh,role=execute]
----
alias velero='oc -n openshift-adp exec deployment/velero -c velero -it -- ./velero'
----
Once you have the alias defined, you can easily get information on the backup and restores resources as well as any of the OADP resources. 

[source,sh,role=execute]
----
velero get backups
----
```
NAME             STATUS      ERRORS   WARNINGS   CREATED                         EXPIRES   STORAGE LOCATION   SELECTOR
fedora01-n2489   Completed   0        0          2024-08-26 22:20:34 +0000 UTC   29d       oadp-dpa-1         <none>
```

Use the `velero describe`` command to retrieve a summary or warnings and errors associtaed with a backup or restore.
[source,sh,role=execute]
----
velero describe backup fedora01-n2489
----
```
Name:         fedora01-n2489
Namespace:    openshift-adp
Labels:       velero.io/storage-location=oadp-dpa-1
Annotations:  velero.io/resource-timeout=10m0s
              velero.io/source-cluster-k8s-gitversion=v1.28.11+add48d0
              velero.io/source-cluster-k8s-major-version=1
              velero.io/source-cluster-k8s-minor-version=28

Phase:  Completed

Namespaces:
  Included:  vmexamples
  Excluded:  <none>

Resources:
  Included:        *
  Excluded:        <none>
  Cluster-scoped:  auto

Label selector:  <none>

Or label selector:  app=fedora-01 or vm.kubevirt.io/name=fedora-01

Storage Location:  oadp-dpa-1

Velero-Native Snapshot PVs:  auto
Snapshot Move Data:          false
Data Mover:                  velero

TTL:  720h0m0s

CSISnapshotTimeout:    10m0s
ItemOperationTimeout:  4h0m0s

Hooks:  <none>

Backup Format Version:  1.1.0

Started:    2024-08-26 22:20:34 +0000 UTC
Completed:  2024-08-26 22:20:59 +0000 UTC

Expiration:  2024-09-25 22:20:34 +0000 UTC

Total items to be backed up:  24
Items backed up:              24

Backup Item Operations:  3 of 3 completed successfully, 0 failed (specify --details for more information)
Backup Volumes:
  Velero-Native Snapshots: <none included>

  CSI Snapshots:
    vmexamples/fedora-01:
      Snapshot: included, specify --details for more information
    vmexamples/fedora-02-appdisk:
      Snapshot: included, specify --details for more information
    vmexamples/restore-be40177b-a972-4566-8bbf-6a5c77093dc3-rootdisk:
      Snapshot: included, specify --details for more information

  Pod Volume Backups: <none included>

HooksAttempted:  4
HooksFailed:     0
```


Use the `velero log`` command to retrieve the logs of a Backup or Restore CR.

[source,sh,role=execute]
----
velero backup logs vmexamples-l6ckl
----

```
... output truncated
time="2024-08-26T22:20:54Z" level=info msg="Found associated CRD virtualmachines.kubevirt.io to add to backup" backup=openshift-adp/fedora01-n2489 logSource="/remote-source/velero/app/pkg/backup/backup.go:581"
time="2024-08-26T22:20:54Z" level=info msg="Backing up item" backup=openshift-adp/fedora01-n2489 logSource="/remote-source/velero/app/pkg/backup/item_backupper.go:176" name=virtualmachines.kubevirt.io namespace= resource=customresourcedefinitions.apiextensions.k8s.io
time="2024-08-26T22:20:54Z" level=info msg="Executing custom action" backup=openshift-adp/fedora01-n2489 logSource="/remote-source/velero/app/pkg/backup/item_backupper.go:363" name=virtualmachines.kubevirt.io namespace= resource=customresourcedefinitions.apiextensions.k8s.io
time="2024-08-26T22:20:54Z" level=info msg="Executing RemapCRDVersionAction" backup=openshift-adp/fedora01-n2489 cmd=/velero logSource="/remote-source/velero/app/pkg/backup/actions/remap_crd_version_action.go:61" pluginName=velero
time="2024-08-26T22:20:54Z" level=info msg="Exiting RemapCRDVersionAction, the cluster does not support v1beta1 CRD" backup=openshift-adp/fedora01-n2489 cmd=/velero logSource="/remote-source/velero/app/pkg/backup/actions/remap_crd_version_action.go:88" pluginName=velero
time="2024-08-26T22:20:54Z" level=info msg="Summary for skipped PVs: []" backup=openshift-adp/fedora01-n2489 logSource="/remote-source/velero/app/pkg/backup/backup.go:494"
time="2024-08-26T22:20:54Z" level=info msg="Backed up a total of 24 items" backup=openshift-adp/fedora01-n2489 logSource="/remote-source/velero/app/pkg/backup/backup.go:498" progress=
```

You can specify the Velero log level in the DataProtectionApplication resource.  This option is available starting from OADP 1.0.3.

----
apiVersion: oadp.openshift.io/v1alpha1
kind: DataProtectionApplication
metadata:
  name: velero-sample
spec:
  configuration:
    velero:
      logLevel: warning
----

The following `logLevel` values are available:  

* trace   
* debug  
* info 
* warning  
* error  
* fatal  
* panic  

It is recommended to use `debug` for most logs.

Deleting a backup or restore CRD must be done through the CLI.  Backups will be deleted when the TTL expires but if you want to delete them prior to the TTL (ie. partially failed), you must use the `velero` CLI to remove the backup from the S3 bucket.  

[source,sh,role=execute]
----
velero delete restore restore-fedora01-8q8zk
----

```
Are you sure you want to continue (Y/N)? y
Request to delete restore "restore-fedora01-8q8zk" submitted successfully.
The restore will be fully deleted after all associated data (restore files in object storage) are removed.
```

[IMPORTANT]
The S3 bucket is the source of truth for the backups.  If a backup or restore CR is deleted using the GUI, when Velero scans the bucket to reconcile the metadata, the CRD will be recreated within the OADP Operator.  This behavior is what also supports the ability to restore resources from one cluster to another.

